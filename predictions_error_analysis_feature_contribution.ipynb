{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "data = pd.read_csv(r'new_removed_tables\\FINAL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.25, random_state=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create an instance of the KNNImputer with desired parameters\n",
    "imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "\n",
    "# Perform KNN imputation on the data\n",
    "imputed_data_train = imputer.fit_transform(train)\n",
    "\n",
    "# Convert the imputed data array back to a DataFrame\n",
    "imputed_train = pd.DataFrame(imputed_data_train, columns=train.columns)\n",
    "\n",
    "# Display the imputed DataFrame\n",
    "imputed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Create an instance of the KNNImputer with desired parameters\n",
    "imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "\n",
    "# Perform KNN imputation on the data\n",
    "imputed_data_test = imputer.fit_transform(test)\n",
    "\n",
    "# Convert the imputed data array back to a DataFrame\n",
    "imputed_test = pd.DataFrame(imputed_data_test, columns=test.columns)\n",
    "\n",
    "# Display the imputed DataFrame\n",
    "imputed_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop the tardet\n",
    "X_train = imputed_train.drop('Average Grades')\n",
    "y_train = imputed_train['Average Grades']\n",
    "\n",
    "X_test = imputed_test.drop('Average Grades')\n",
    "y_test =imputed_test['Average Grades']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Create an instance of MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "# Perform normalization on the selected columns\n",
    "X_train  = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, LeaveOneOut, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "model_lr = LinearRegression()\n",
    "\n",
    "# Define the scoring metric\n",
    "scorer = make_scorer(mean_absolute_error)\n",
    "\n",
    "# Create a GridSearchCV object with LOOCV\n",
    "\n",
    "# Initialize variables to store the results\n",
    "mae_scores = []\n",
    "\n",
    "for train_index, test_index in loo.split(X_train):\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # Fit the linear regression model on the training data\n",
    "    model_lr.fit(X_train_cv, y_train_cv)\n",
    "    \n",
    "    # Use the fitted model to predict the value of the left-out data point\n",
    "    y_pred_LR_CV = model_lr.predict(X_test_cv)\n",
    "    \n",
    "    # Calculate the mean squared error of the prediction\n",
    "    mae = mean_absolute_error(y_test_cv, y_pred_LR_CV)\n",
    "    \n",
    "    # Append the MSE score to a list\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "# Calculate the average MSE over all data points\n",
    "avg_mae = np.mean(mae_scores)\n",
    "\n",
    "# Print the average MSE score\n",
    "print(\"Average MAE score:\", avg_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import pickle\n",
    "\n",
    "path = r'new_datasets\\models\\Linear_regression.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(model_lr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on unseen data\n",
    "\n",
    "lr_pred = model_lr.predict(X_test)\n",
    "\n",
    "#MAE score\n",
    "mae = mean_absolute_error(y_test, lr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual Vs. Predicted Grades\n",
    "\n",
    "# Reshape y_test to match the shape of lr\n",
    "y_test_reshaped = np.array(y_test).reshape((1, -1))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(y_test_reshaped, model_lr, color='blue', alpha=0.5)\n",
    "\n",
    "# Add a diagonal line indicating perfect predictions\n",
    "plt.plot([min(y_test_reshaped), max(y_test_reshaped)], [min(y_test_reshaped), max(y_test_reshaped)], color='red')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Actual Grades vs. Linear Regression Predicted Grades')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histplot of The actudal and the predicted grades\n",
    "\n",
    "\n",
    "sns.histplot(y_test, label='Ground Truth', kde=True, line_kws={'color': 'green'})\n",
    "sns.histplot(model_lr, label='Linear Regression Predictions', kde=True, line_kws={'color': 'red'})\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Ground Truth vs. Linear Regression Predictions')\n",
    "\n",
    "# Set the y-axis tick locations and labels\n",
    "plt.xticks([i/2 for i in range(21)])\n",
    "\n",
    "plt.xlim(4.0, 11)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 score \n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Calculate the R-squared value\n",
    "r2 = r2_score(y_test, model_lr)\n",
    "\n",
    "# Print the R-squared value\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "outer_cv = LeaveOneOut()\n",
    "\n",
    "\n",
    "outer_results = list()\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X_train):\n",
    "    #split data\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    inner_cv = KFold()\n",
    "    model_svr = SVR()\n",
    "    parameters = {'kernel': ('rbf','poly'), 'C':[1, 1.5, 10],'gamma': [1e-7, 1e-4],\n",
    "                                                            'epsilon':[0.1,0.2,0.5,0.3]}\n",
    "    score = make_scorer(mean_absolute_error)\n",
    "    \n",
    "    #define search\n",
    "    search = GridSearchCV(model_svr, parameters, scoring=score, cv=inner_cv)\n",
    "    \n",
    "    #result\n",
    "    result = search.fit(X_train_cv, np.ravel(y_train_cv))\n",
    "\n",
    "    #get the best performing fit\n",
    "    best_model_svr = result.best_estimator_\n",
    "\n",
    "    #evaluate the model on the hold out dataset\n",
    "    eval = best_model_svr.predict(X_test_cv)\n",
    "\n",
    "    mae = mean_absolute_error(y_test_cv, eval)\n",
    "\n",
    "    #store the results\n",
    "\n",
    "    outer_results.append(mae)\n",
    "\n",
    "    print('>mae=%.3f, est=%.3f, cfg=%s' % (mae, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Mean Absolute Error: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = r'new_datasets\\models\\SVR_model.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(best_model_svr, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "\n",
    "svr_pred = best_model_svr.predict(X_test)\n",
    "\n",
    "#MAE score\n",
    "mae = mean_absolute_error(y_test, svr_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression plot of the predictions and the true values\n",
    "\n",
    "# Reshape y_test to match the shape of svr\n",
    "y_test_reshaped = np.array(y_test).reshape((1, -1))\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(y_test_reshaped, best_model_svr, color='blue', alpha=0.5)\n",
    "\n",
    "# Add a diagonal line indicating perfect predictions\n",
    "plt.plot([min(y_test_reshaped[0]), max(y_test_reshaped[0])], [min(y_test_reshaped[0]), max(y_test_reshaped[0])], color='red')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Actual Grades vs. Support Vector Regression Predicted Grades')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histplot of the tredicted and true values\n",
    "sns.histplot(y_test, color='blue', label='Ground Truth', kde=True)\n",
    "sns.histplot(best_model_svr, color='red', label='Support Vector Regression Predictions', kde=True)\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Ground Truth vs. Support Vector Regression Predictions')\n",
    "\n",
    "# Set the y-axis tick locations and labels\n",
    "plt.xticks([i/2 for i in range(21)])\n",
    "\n",
    "plt.xlim(4.0, 9.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the R-squared value\n",
    "r2 = r2_score(y_test, best_model_svr)\n",
    "\n",
    "# Print the R-squared value\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decission Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X_train):\n",
    "    # Split data\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    cv = KFold()\n",
    "    tree_cv = DecisionTreeRegressor()\n",
    "    parameters = {\n",
    "        \"splitter\": [\"best\", \"random\"],\n",
    "        \"max_depth\": [3, 7, 11, 12],\n",
    "        \"min_samples_leaf\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        \"min_weight_fraction_leaf\": [0.1, 0.2],\n",
    "        \"max_features\": [\"auto\", \"log2\", \"sqrt\", None],\n",
    "        \"max_leaf_nodes\": [10, 20, 40, 50, 90]\n",
    "    }\n",
    "    score = make_scorer(mean_absolute_error)\n",
    "    \n",
    "    # Define search\n",
    "    search = GridSearchCV(tree_cv, parameters, scoring=score, cv=cv)\n",
    "    \n",
    "    # Result\n",
    "    result = search.fit(X_train_cv, np.ravel(y_train_cv))\n",
    "\n",
    "    # Get the best performing fit\n",
    "    best_model_dt= result.best_estimator_\n",
    "\n",
    "    # Evaluate the model on the holdout dataset\n",
    "    eval = best_model_dt.predict(X_test_cv)\n",
    "\n",
    "    mae = mean_absolute_error(y_test_cv, eval)\n",
    "\n",
    "    # Store the results\n",
    "    outer_results.append(mae)\n",
    "\n",
    "    print('> MAE=%.3f, Est=%.3f, Cfg=%s' % (mae, result.best_score_, result.best_params_))\n",
    "\n",
    "# Summarize the estimated performance of the model\n",
    "print('Mean Absolute Error: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "\n",
    "dt_pred = best_model_dt.predict(X_test)\n",
    "\n",
    "#MAE score\n",
    "mae = mean_absolute_error(y_test, dt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = r'new_datasets\\models\\DT_model.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(best_model_dt, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot\n",
    "plt.scatter(y_test_reshaped, best_model_dt, color='blue', alpha=0.5)\n",
    "\n",
    "# Add a diagonal line indicating perfect predictions\n",
    "plt.plot([min(y_test_reshaped), max(y_test_reshaped)], [min(y_test_reshaped), max(y_test_reshaped)], color='red')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Actual Grades vs. Decision Tree Predicted Grades')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histplot of predicted and true values\n",
    "\n",
    "sns.histplot(y_test, color='blue', label='Ground Truth', kde=True)\n",
    "sns.histplot(best_model_dt, color='red', label='Decision Tree Predictions', kde=True)\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Ground Truth vs. Decision Tree Predictions')\n",
    "\n",
    "# Set the y-axis tick locations and labels\n",
    "plt.xticks([i/2 for i in range(21)])\n",
    "\n",
    "plt.xlim(4.0, 9.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#r2\n",
    "r2 = r2_score(y_test_reshaped, best_model_dt)\n",
    "\n",
    "# Print the R-squared value\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "outer_results = list()\n",
    "\n",
    "outer_cv = LeaveOneOut()\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X_train):\n",
    "    #split data\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    inner_cv = KFold()\n",
    "    rf_cv = RandomForestRegressor()\n",
    "\n",
    "    param_grid = {\n",
    "    'n_estimators': [100, 200, 300], \n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]}\n",
    "        \n",
    "    score = make_scorer(mean_absolute_error)\n",
    "    \n",
    "    #define search\n",
    "    search = GridSearchCV(rf_cv, param_grid, scoring=score, cv=inner_cv)\n",
    "    \n",
    "    #result\n",
    "    result = search.fit(X_train_cv, np.ravel(y_train_cv))\n",
    "\n",
    "    #get the best performing fit\n",
    "    best_model_rf = result.best_estimator_\n",
    "\n",
    "    #evaluate the model on the hold out dataset\n",
    "    eval = best_model_rf.predict(X_test_cv)\n",
    "\n",
    "    mae = mean_absolute_error(y_test_cv, eval)\n",
    "\n",
    "    #store the results\n",
    "\n",
    "    outer_results.append(mae)\n",
    "\n",
    "    print('>mae=%.3f, est=%.3f, cfg=%s' % (mae, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Mean Absolute Error: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "\n",
    "rf_pred = best_model_rf.predict(X_test)\n",
    "\n",
    "#MAE score\n",
    "mae = mean_absolute_error(y_test, rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = r'new_datasets\\models\\RF_model.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(best_model_rf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression plot of true and predicted values\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(y_test_reshaped, best_model_rf, color='blue', alpha=0.5)\n",
    "\n",
    "# Add a diagonal line indicating perfect predictions\n",
    "plt.plot([min(y_test_reshaped[0]), max(y_test_reshaped[0])], [min(y_test_reshaped[0]), max(y_test_reshaped[0])], color='red')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Actual Grades vs. Random Forest Predicted Grades')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histplot of true and predicted values \n",
    "\n",
    "sns.histplot(y_test, color='blue', label='Ground Truth', kde=True)\n",
    "sns.histplot(best_model_rf, color='red', label='Random Forest Predictions', kde=True)\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Ground Truth vs. Random Forest Predictions')\n",
    "\n",
    "# Set the y-axis tick locations and labels\n",
    "plt.xticks([i/2 for i in range(21)])\n",
    "\n",
    "plt.xlim(4.0, 9.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the R-squared value\n",
    "r2 = r2_score(y_test, best_model_rf)\n",
    "\n",
    "# Print the R-squared value\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "outer_results = list()\n",
    "\n",
    "outer_cv = LeaveOneOut()\n",
    "\n",
    "for train_index, test_index in outer_cv.split(X_train):\n",
    "    #split data\n",
    "    X_train_cv, X_test_cv = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "    y_train_cv, y_test_cv = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "    inner_cv = KFold()\n",
    "    xgboost_cv = XGBRegressor()\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.1, 0.3],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.6, 0.8],\n",
    "        'colsample_bytree': [0.6, 0.8]\n",
    "                                        }\n",
    "    \n",
    "    score = make_scorer(mean_absolute_error)\n",
    "    \n",
    "    #define search\n",
    "    search = GridSearchCV(xgboost_cv, param_grid, scoring=score, cv=inner_cv)\n",
    "    \n",
    "    #result\n",
    "    result = search.fit(X_train_cv, y_train_cv)\n",
    "\n",
    "    #get the best performing fit\n",
    "    best_model_xgb = result.best_estimator_\n",
    "\n",
    "    #evaluate the model on the hold out dataset\n",
    "    eval = best_model_xgb.predict(X_test_cv)\n",
    "\n",
    "    mae = mean_absolute_error(y_test_cv, eval)\n",
    "\n",
    "    #store the results\n",
    "\n",
    "    outer_results.append(mae)\n",
    "\n",
    "    print('>mae=%.3f, est=%.3f, cfg=%s' % (mae, result.best_score_, result.best_params_))\n",
    "# summarize the estimated performance of the model\n",
    "print('Mean Absolute Error: %.3f (%.3f)' % (np.mean(outer_results), np.std(outer_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on test set\n",
    "\n",
    "xgb_pred = best_model_xgb.predict(X_test)\n",
    "\n",
    "#MAE score\n",
    "mae = mean_absolute_error(y_test, xgb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "import pickle\n",
    "\n",
    "path = r'new_datasets\\models\\XGB_model.pkl'\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(best_model_xgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression plot of true and predicted values\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.scatter(y_test_reshaped, best_model_xgb, color='blue', alpha=0.5)\n",
    "\n",
    "# Add a diagonal line indicating perfect predictions\n",
    "plt.plot([min(y_test_reshaped), max(y_test_reshaped)], [min(y_test_reshaped), max(y_test_reshaped)], color='red')\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Actual Grades')\n",
    "plt.ylabel('Predicted Grades')\n",
    "plt.title('Actual Grades vs. XGBoost Predicted Grades')\n",
    "\n",
    "# Display the\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histplot of true and predicted values\n",
    "\n",
    "sns.histplot(y_test, color='blue', label='Ground Truth', kde=True)\n",
    "sns.histplot(best_model_xgb, color='red', label='Xgboost Predictions', kde=True)\n",
    "\n",
    "# Set plot labels and title\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Ground Truth vs. Xgboost Predictions')\n",
    "\n",
    "# Set the y-axis tick locations and labels\n",
    "plt.xticks([i/2 for i in range(21)])\n",
    "\n",
    "plt.xlim(4.0, 9.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_test_reshaped, xgb)\n",
    "\n",
    "# Print the R-squared value\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictor's contribution - SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.Explainer(best_model_rf)\n",
    "\n",
    "# Calculate the SHAP values for all instances in the test set\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Get the underlying values from the Explanation object\n",
    "shap_values_values = shap_values.values\n",
    "\n",
    "# Calculate the mean absolute SHAP values for each feature\n",
    "mean_abs_shap_values = np.abs(shap_values_values).mean(axis=0)\n",
    "\n",
    "# Find the indices of the top 20 most influential values\n",
    "top_20_indices = np.argsort(mean_abs_shap_values)[-20:]\n",
    "\n",
    "# Get the corresponding feature names or values\n",
    "top_20_features = X_test.columns[top_20_indices]  \n",
    "top_20_values = mean_abs_shap_values[top_20_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 20 SHAP values with corrected names\n",
    "\n",
    "# Define the custom feature names for the y-axis labels\n",
    "feature_names = ['Ratio Phone Usage February daily window 24-6', 'N days campus visited February', 'Ratio Phone Usage June daily window 0am-6am', 'Ratio Phone Usage January daily window 12-18', 'Ratio Phone Usage May daily window 24-6', 'Stress trend slope March', 'Ratio Phone Usage May daily window 12-18', 'Energy trend slope March', 'Average time studying February', 'Relaxed trend slope June', 'Average time studying February April', 'Ratio Phone Usage January daily window 24-6', 'Overcoming difficulties', 'Stress overall slope', 'Average hours slept February', 'N days campus visited January', 'Concentration trend slope March', 'Ratio Phone Usage February daily window 12-18', 'Concentration trend slope June', 'Total time spent on campus January']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the top 20 most influential values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(top_20_features)), top_20_values, align='center')\n",
    "plt.yticks(range(len(top_20_features)), feature_names)\n",
    "plt.xlabel('Mean Absolute SHAP Value')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top 20 Most Influential Values')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
